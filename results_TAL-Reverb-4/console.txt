Folder:  models\03-06Rev_1
Fold batches: 2853 714
Model: RevNetGLU(
  (conv1): Conv1d(1, 1024, kernel_size=(512,), stride=(4,), padding=(256,))
  (act1): GLU(dim=-2)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop1): Dropout(p=0.3, inplace=False)
  (conv2): Conv1d(512, 128, kernel_size=(64,), stride=(1,), padding=(32,))
  (act2): GLU(dim=-2)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop2): Dropout(p=0.3, inplace=False)
  (conv3): Conv1d(64, 128, kernel_size=(64,), stride=(1,), padding=(32,))
  (act3): GLU(dim=-2)
  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop3): Dropout(p=0.3, inplace=False)
  (conv4): Conv1d(64, 128, kernel_size=(64,), stride=(1,), padding=(32,))
  (act4): GLU(dim=-2)
  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop4): Dropout(p=0.3, inplace=False)
  (conv5): Conv1d(64, 256, kernel_size=(64,), stride=(1,), padding=(32,))
  (act5): GLU(dim=-2)
  (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool5): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop5): Dropout(p=0.3, inplace=False)
  (conv6): Conv1d(128, 1536, kernel_size=(64,), stride=(1,), padding=(32,))
  (act6): GLU(dim=-2)
  (bn6): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool6): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
  (drop6): Dropout(p=0.3, inplace=False)
  (lin5o): ModuleList(
    (0): Linear(in_features=768, out_features=256, bias=True)
    (1): Linear(in_features=768, out_features=256, bias=True)
    (2): Linear(in_features=768, out_features=256, bias=True)
  )
  (norm5o): ModuleList(
    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drop5o): ModuleList(
    (0): Dropout(p=0.3, inplace=False)
    (1): Dropout(p=0.3, inplace=False)
    (2): Dropout(p=0.3, inplace=False)
  )
  (lin6o): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Linear(in_features=256, out_features=128, bias=True)
    (2): Linear(in_features=256, out_features=128, bias=True)
  )
  (norm6o): ModuleList(
    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (drop6o): ModuleList(
    (0): Dropout(p=0.3, inplace=False)
    (1): Dropout(p=0.3, inplace=False)
    (2): Dropout(p=0.3, inplace=False)
  )
  (linout): ModuleList(
    (0): Linear(in_features=128, out_features=1, bias=True)
    (1): Linear(in_features=128, out_features=1, bias=True)
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
Speech samples: 57076
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1          [-1, 1024, 20001]         525,312
               GLU-2           [-1, 512, 20001]               0
       BatchNorm1d-3           [-1, 512, 20001]           1,024
         MaxPool1d-4            [-1, 512, 5000]               0
           Dropout-5            [-1, 512, 5000]               0
            Conv1d-6            [-1, 128, 5001]       4,194,432
               GLU-7             [-1, 64, 5001]               0
       BatchNorm1d-8             [-1, 64, 5001]             128
         MaxPool1d-9             [-1, 64, 1250]               0
          Dropout-10             [-1, 64, 1250]               0
           Conv1d-11            [-1, 128, 1251]         524,416
              GLU-12             [-1, 64, 1251]               0
      BatchNorm1d-13             [-1, 64, 1251]             128
        MaxPool1d-14              [-1, 64, 312]               0
          Dropout-15              [-1, 64, 312]               0
           Conv1d-16             [-1, 128, 313]         524,416
              GLU-17              [-1, 64, 313]               0
      BatchNorm1d-18              [-1, 64, 313]             128
        MaxPool1d-19               [-1, 64, 78]               0
          Dropout-20               [-1, 64, 78]               0
           Conv1d-21              [-1, 256, 79]       1,048,832
              GLU-22              [-1, 128, 79]               0
      BatchNorm1d-23              [-1, 128, 79]             256
        MaxPool1d-24              [-1, 128, 19]               0
          Dropout-25              [-1, 128, 19]               0
           Conv1d-26             [-1, 1536, 20]      12,584,448
              GLU-27              [-1, 768, 20]               0
      BatchNorm1d-28              [-1, 768, 20]           1,536
        MaxPool1d-29               [-1, 768, 5]               0
          Dropout-30               [-1, 768, 5]               0
           Linear-31                  [-1, 256]         196,864
          Dropout-32                  [-1, 256]               0
           Linear-33                  [-1, 128]          32,896
          Dropout-34                  [-1, 128]               0
           Linear-35                    [-1, 1]             129
           Linear-36                  [-1, 256]         196,864
          Dropout-37                  [-1, 256]               0
           Linear-38                  [-1, 128]          32,896
          Dropout-39                  [-1, 128]               0
           Linear-40                    [-1, 1]             129
           Linear-41                  [-1, 256]         196,864
          Dropout-42                  [-1, 256]               0
           Linear-43                  [-1, 128]          32,896
          Dropout-44                  [-1, 128]               0
           Linear-45                    [-1, 1]             129
================================================================
Total params: 20,094,723
Trainable params: 20,094,723
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.31
Forward/backward pass size (MB): 366.89
Params size (MB): 76.66
Estimated Total Size (MB): 443.85
----------------------------------------------------------------
11:23:47 Training...
877s - epoch:  1/100 - loss: 0.1804 - val_loss: 0.1401 - train_dist: 0.180 - val_dist: 0.140 - val_R²: 0.57
876s - epoch:  2/100 - loss: 0.1374 - val_loss: 0.1047 - train_dist: 0.137 - val_dist: 0.105 - val_R²: 0.72
876s - epoch:  3/100 - loss: 0.1204 - val_loss: 0.0997 - train_dist: 0.120 - val_dist: 0.100 - val_R²: 0.73
875s - epoch:  4/100 - loss: 0.1104 - val_loss: 0.0983 - train_dist: 0.110 - val_dist: 0.098 - val_R²: 0.73
876s - epoch:  5/100 - loss: 0.1035 - val_loss: 0.0982 - train_dist: 0.104 - val_dist: 0.098 - val_R²: 0.73
876s - epoch:  6/100 - loss: 0.0980 - val_loss: 0.0813 - train_dist: 0.098 - val_dist: 0.081 - val_R²: 0.79
876s - epoch:  7/100 - loss: 0.0940 - val_loss: 0.0820 - train_dist: 0.094 - val_dist: 0.082 - val_R²: 0.78
876s - epoch:  8/100 - loss: 0.0911 - val_loss: 0.0768 - train_dist: 0.091 - val_dist: 0.077 - val_R²: 0.80
875s - epoch:  9/100 - loss: 0.0879 - val_loss: 0.0751 - train_dist: 0.088 - val_dist: 0.075 - val_R²: 0.80
875s - epoch: 10/100 - loss: 0.0853 - val_loss: 0.0739 - train_dist: 0.085 - val_dist: 0.074 - val_R²: 0.81
875s - epoch: 11/100 - loss: 0.0832 - val_loss: 0.0710 - train_dist: 0.083 - val_dist: 0.071 - val_R²: 0.82
875s - epoch: 12/100 - loss: 0.0814 - val_loss: 0.0713 - train_dist: 0.081 - val_dist: 0.071 - val_R²: 0.82
875s - epoch: 13/100 - loss: 0.0799 - val_loss: 0.0726 - train_dist: 0.080 - val_dist: 0.073 - val_R²: 0.81
874s - epoch: 14/100 - loss: 0.0782 - val_loss: 0.0757 - train_dist: 0.078 - val_dist: 0.076 - val_R²: 0.81
875s - epoch: 15/100 - loss: 0.0766 - val_loss: 0.0684 - train_dist: 0.077 - val_dist: 0.068 - val_R²: 0.83
875s - epoch: 16/100 - loss: 0.0753 - val_loss: 0.0709 - train_dist: 0.075 - val_dist: 0.071 - val_R²: 0.82
874s - epoch: 17/100 - loss: 0.0739 - val_loss: 0.0722 - train_dist: 0.074 - val_dist: 0.072 - val_R²: 0.81
875s - epoch: 18/100 - loss: 0.0733 - val_loss: 0.0691 - train_dist: 0.073 - val_dist: 0.069 - val_R²: 0.83
875s - epoch: 19/100 - loss: 0.0722 - val_loss: 0.0759 - train_dist: 0.072 - val_dist: 0.076 - val_R²: 0.80
875s - epoch: 20/100 - loss: 0.0713 - val_loss: 0.0681 - train_dist: 0.071 - val_dist: 0.068 - val_R²: 0.83
874s - epoch: 21/100 - loss: 0.0702 - val_loss: 0.0680 - train_dist: 0.070 - val_dist: 0.068 - val_R²: 0.83
875s - epoch: 22/100 - loss: 0.0693 - val_loss: 0.0693 - train_dist: 0.069 - val_dist: 0.069 - val_R²: 0.82
875s - epoch: 23/100 - loss: 0.0687 - val_loss: 0.0682 - train_dist: 0.069 - val_dist: 0.068 - val_R²: 0.83
875s - epoch: 24/100 - loss: 0.0676 - val_loss: 0.0699 - train_dist: 0.068 - val_dist: 0.070 - val_R²: 0.82
875s - epoch: 25/100 - loss: 0.0668 - val_loss: 0.0664 - train_dist: 0.067 - val_dist: 0.066 - val_R²: 0.83
875s - epoch: 26/100 - loss: 0.0663 - val_loss: 0.0653 - train_dist: 0.066 - val_dist: 0.065 - val_R²: 0.83
876s - epoch: 27/100 - loss: 0.0658 - val_loss: 0.0684 - train_dist: 0.066 - val_dist: 0.068 - val_R²: 0.82
875s - epoch: 28/100 - loss: 0.0649 - val_loss: 0.0669 - train_dist: 0.065 - val_dist: 0.067 - val_R²: 0.83
876s - epoch: 29/100 - loss: 0.0643 - val_loss: 0.0659 - train_dist: 0.064 - val_dist: 0.066 - val_R²: 0.83
875s - epoch: 30/100 - loss: 0.0637 - val_loss: 0.0656 - train_dist: 0.064 - val_dist: 0.066 - val_R²: 0.83
875s - epoch: 31/100 - loss: 0.0632 - val_loss: 0.0661 - train_dist: 0.063 - val_dist: 0.066 - val_R²: 0.83
875s - epoch: 32/100 - loss: 0.0625 - val_loss: 0.0675 - train_dist: 0.062 - val_dist: 0.068 - val_R²: 0.82
875s - epoch: 33/100 - loss: 0.0619 - val_loss: 0.0688 - train_dist: 0.062 - val_dist: 0.069 - val_R²: 0.82
875s - epoch: 34/100 - loss: 0.0615 - val_loss: 0.0697 - train_dist: 0.062 - val_dist: 0.070 - val_R²: 0.82
875s - epoch: 35/100 - loss: 0.0610 - val_loss: 0.0664 - train_dist: 0.061 - val_dist: 0.066 - val_R²: 0.83
875s - epoch: 36/100 - loss: 0.0604 - val_loss: 0.0688 - train_dist: 0.060 - val_dist: 0.069 - val_R²: 0.81
875s - epoch: 37/100 - loss: 0.0600 - val_loss: 0.0665 - train_dist: 0.060 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 38/100 - loss: 0.0593 - val_loss: 0.0650 - train_dist: 0.059 - val_dist: 0.065 - val_R²: 0.82
875s - epoch: 39/100 - loss: 0.0589 - val_loss: 0.0663 - train_dist: 0.059 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 40/100 - loss: 0.0586 - val_loss: 0.0660 - train_dist: 0.059 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 41/100 - loss: 0.0582 - val_loss: 0.0664 - train_dist: 0.058 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 42/100 - loss: 0.0574 - val_loss: 0.0662 - train_dist: 0.057 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 43/100 - loss: 0.0573 - val_loss: 0.0664 - train_dist: 0.057 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 44/100 - loss: 0.0571 - val_loss: 0.0657 - train_dist: 0.057 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 45/100 - loss: 0.0567 - val_loss: 0.0666 - train_dist: 0.057 - val_dist: 0.067 - val_R²: 0.82
875s - epoch: 46/100 - loss: 0.0563 - val_loss: 0.0633 - train_dist: 0.056 - val_dist: 0.063 - val_R²: 0.82
875s - epoch: 47/100 - loss: 0.0558 - val_loss: 0.0697 - train_dist: 0.056 - val_dist: 0.070 - val_R²: 0.81
876s - epoch: 48/100 - loss: 0.0558 - val_loss: 0.0646 - train_dist: 0.056 - val_dist: 0.065 - val_R²: 0.82
876s - epoch: 49/100 - loss: 0.0555 - val_loss: 0.0656 - train_dist: 0.056 - val_dist: 0.066 - val_R²: 0.81
874s - epoch: 50/100 - loss: 0.0553 - val_loss: 0.0638 - train_dist: 0.055 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 51/100 - loss: 0.0544 - val_loss: 0.0680 - train_dist: 0.054 - val_dist: 0.068 - val_R²: 0.81
875s - epoch: 52/100 - loss: 0.0546 - val_loss: 0.0645 - train_dist: 0.055 - val_dist: 0.065 - val_R²: 0.82
876s - epoch: 53/100 - loss: 0.0541 - val_loss: 0.0662 - train_dist: 0.054 - val_dist: 0.066 - val_R²: 0.82
875s - epoch: 54/100 - loss: 0.0541 - val_loss: 0.0641 - train_dist: 0.054 - val_dist: 0.064 - val_R²: 0.81
874s - epoch: 55/100 - loss: 0.0536 - val_loss: 0.0669 - train_dist: 0.054 - val_dist: 0.067 - val_R²: 0.81
874s - epoch: 56/100 - loss: 0.0534 - val_loss: 0.0674 - train_dist: 0.053 - val_dist: 0.067 - val_R²: 0.81
875s - epoch: 57/100 - loss: 0.0530 - val_loss: 0.0634 - train_dist: 0.053 - val_dist: 0.063 - val_R²: 0.82
874s - epoch: 58/100 - loss: 0.0531 - val_loss: 0.0653 - train_dist: 0.053 - val_dist: 0.065 - val_R²: 0.81
875s - epoch: 59/100 - loss: 0.0525 - val_loss: 0.0641 - train_dist: 0.052 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 60/100 - loss: 0.0523 - val_loss: 0.0640 - train_dist: 0.052 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 61/100 - loss: 0.0522 - val_loss: 0.0634 - train_dist: 0.052 - val_dist: 0.063 - val_R²: 0.83
875s - epoch: 62/100 - loss: 0.0521 - val_loss: 0.0640 - train_dist: 0.052 - val_dist: 0.064 - val_R²: 0.81
874s - epoch: 63/100 - loss: 0.0515 - val_loss: 0.0644 - train_dist: 0.052 - val_dist: 0.064 - val_R²: 0.83
875s - epoch: 64/100 - loss: 0.0514 - val_loss: 0.0636 - train_dist: 0.051 - val_dist: 0.064 - val_R²: 0.82
874s - epoch: 65/100 - loss: 0.0513 - val_loss: 0.0623 - train_dist: 0.051 - val_dist: 0.062 - val_R²: 0.82
875s - epoch: 66/100 - loss: 0.0511 - val_loss: 0.0617 - train_dist: 0.051 - val_dist: 0.062 - val_R²: 0.82
875s - epoch: 67/100 - loss: 0.0509 - val_loss: 0.0644 - train_dist: 0.051 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 68/100 - loss: 0.0507 - val_loss: 0.0626 - train_dist: 0.051 - val_dist: 0.063 - val_R²: 0.82
874s - epoch: 69/100 - loss: 0.0505 - val_loss: 0.0654 - train_dist: 0.051 - val_dist: 0.065 - val_R²: 0.82
874s - epoch: 70/100 - loss: 0.0511 - val_loss: 0.0645 - train_dist: 0.051 - val_dist: 0.064 - val_R²: 0.81
875s - epoch: 71/100 - loss: 0.0501 - val_loss: 0.0655 - train_dist: 0.050 - val_dist: 0.066 - val_R²: 0.82
874s - epoch: 72/100 - loss: 0.0503 - val_loss: 0.0632 - train_dist: 0.050 - val_dist: 0.063 - val_R²: 0.82
875s - epoch: 73/100 - loss: 0.0496 - val_loss: 0.0650 - train_dist: 0.050 - val_dist: 0.065 - val_R²: 0.81
875s - epoch: 74/100 - loss: 0.0498 - val_loss: 0.0623 - train_dist: 0.050 - val_dist: 0.062 - val_R²: 0.82
875s - epoch: 75/100 - loss: 0.0496 - val_loss: 0.0667 - train_dist: 0.050 - val_dist: 0.067 - val_R²: 0.81
875s - epoch: 76/100 - loss: 0.0495 - val_loss: 0.0639 - train_dist: 0.049 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 77/100 - loss: 0.0493 - val_loss: 0.0652 - train_dist: 0.049 - val_dist: 0.065 - val_R²: 0.82
875s - epoch: 78/100 - loss: 0.0490 - val_loss: 0.0618 - train_dist: 0.049 - val_dist: 0.062 - val_R²: 0.83
875s - epoch: 79/100 - loss: 0.0486 - val_loss: 0.0638 - train_dist: 0.049 - val_dist: 0.064 - val_R²: 0.82
875s - epoch: 80/100 - loss: 0.0484 - val_loss: 0.0617 - train_dist: 0.048 - val_dist: 0.062 - val_R²: 0.83
874s - epoch: 81/100 - loss: 0.0485 - val_loss: 0.0622 - train_dist: 0.049 - val_dist: 0.062 - val_R²: 0.82
875s - epoch: 82/100 - loss: 0.0480 - val_loss: 0.0614 - train_dist: 0.048 - val_dist: 0.061 - val_R²: 0.83
874s - epoch: 83/100 - loss: 0.0481 - val_loss: 0.0611 - train_dist: 0.048 - val_dist: 0.061 - val_R²: 0.82
874s - epoch: 84/100 - loss: 0.0479 - val_loss: 0.0622 - train_dist: 0.048 - val_dist: 0.062 - val_R²: 0.82
874s - epoch: 85/100 - loss: 0.0478 - val_loss: 0.0619 - train_dist: 0.048 - val_dist: 0.062 - val_R²: 0.82
875s - epoch: 86/100 - loss: 0.0477 - val_loss: 0.0624 - train_dist: 0.048 - val_dist: 0.062 - val_R²: 0.82
Early stopping!
08:48:40 Evaluating... 
Test Results:
  Parameter       MAE        R²
0      size  0.047569  0.877223
1       wet  0.060986  0.856861
2   diffuse  0.090789  0.732063
08:51:53 Evaluating in Noisy conditions...
Test Results: 0 dB SNR
  Parameter       MAE        R²
0      size  0.189896  0.083061
1       wet  0.164222  0.359418
2   diffuse  0.204954  0.063556
Test Results: 20 dB SNR
  Parameter       MAE        R²
0      size  0.069081  0.760971
1       wet  0.075134  0.806717
2   diffuse  0.099344  0.693220
Test Results: 40 dB SNR
  Parameter       MAE        R²
0      size  0.051135  0.851625
1       wet  0.066417  0.836274
2   diffuse  0.088735  0.737219
Test Results: 60 dB SNR
  Parameter       MAE        R²
0      size  0.049193  0.864783
1       wet  0.062000  0.842899
2   diffuse  0.088602  0.750573
Test Results: 80 dB SNR
  Parameter       MAE        R²
0      size  0.049121  0.861870
1       wet  0.061985  0.841690
2   diffuse  0.091465  0.733034
Saving: models\03-06Rev_1\plots\size.pdf
Saving: models\03-06Rev_1\plots\wet.pdf
Saving: models\03-06Rev_1\plots\diffuse.pdf
Saving: models\03-06Rev_1\plots\embeddings.pdf
